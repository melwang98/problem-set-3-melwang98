{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating functional connectivity from resting fMRI data\n",
    "\n",
    "In this notebook we will examine how to estimate functional connectivity from resting fMRI data, using data from the Midnight Scan Club project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy,pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import nilearn.plotting\n",
    "import nilearn.input_data\n",
    "import nilearn.datasets\n",
    "import nilearn.connectome\n",
    "import nibabel\n",
    "from nibabel.gifti.gifti import GiftiImage,GiftiDataArray\n",
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "import networkx as nx\n",
    "import bct\n",
    "import pkg_resources\n",
    "\n",
    "from brainnetworks.utils import nx_to_igraph\n",
    "\n",
    "DATA_PATH = pkg_resources.resource_filename('brainnetworks',\n",
    "                                            'data/')\n",
    "                                            \n",
    "%matplotlib inline\n",
    "\n",
    "datadir = nilearn.datasets.get_data_dirs()[0]\n",
    "\n",
    "\n",
    "sub=1\n",
    "ses=1\n",
    "\n",
    "subdir=os.path.join(datadir,'MSC/ds000224/derivatives/fmriprep/sub-MSC%02d/'%sub)\n",
    "assert os.path.exists(subdir)\n",
    "\n",
    "sesdir=os.path.join(subdir,'ses-func%02d/func'%ses)\n",
    "\n",
    "\n",
    "# get freesurfer data if we don't already have it\n",
    "\n",
    "fsaverage = nilearn.datasets.fetch_surf_fsaverage5()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the data that were processed using [fMRIPrep](http://fmriprep.org) for a single session.  This preprocessing include:\n",
    "\n",
    "- motion correction\n",
    "- spatial distortion correction\n",
    "- spatial normalization to the MNI volume template and the FreeSurfer fsaverage5 template\n",
    "\n",
    "We are going to use the data on the surface because we can then easily sample the data into the Glasser et al. MMP parcellation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed fMRI data\n",
    "\n",
    "hemispheres=['L','R']\n",
    "bold_origfile={}\n",
    "ntp=818\n",
    "nverts=10242  # number of vertices in fsaverage5 per hemisphere\n",
    "bolddata_orig=numpy.zeros((ntp,nverts*2))\n",
    "for i,h in enumerate(hemispheres):\n",
    "    bold_origfile[h]=os.path.join(sesdir,'sub-MSC01_ses-func%02d_task-rest_bold_space-fsaverage5.%s.func.gii'%(ses,h))\n",
    "    d=nibabel.load(bold_origfile[h]).darrays\n",
    "    for tp in range(len(d)):\n",
    "        bolddata_orig[tp,(i*nverts):((i+1)*nverts)]=d[tp].data\n",
    "    \n",
    "print('data shape:',bolddata_orig.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound regression\n",
    "\n",
    "One of the common procedures applied to resting fMRI data is to regress out potential confounding variables. These include variables related to motion estimates and their temporal derivatives, as well as variables related to other signal components that are thought to be artifactual.  These can include:\n",
    "\n",
    "- CSF signal\n",
    "- white matter signal\n",
    "- DVARS: a measure of changes in the global brain signal\n",
    "- aCompCor: low-dimensional components within regions defined as non-GM (i.e. CSF and white matter)\n",
    "- tCompCor: low-dimensional components within regions defined by having high temporal variability\n",
    "- low frequency signals (usually below about .08 Hz)\n",
    "\n",
    "These confounds are computed by fMRIprep and saved to a file alongside the data, which we load here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the confound data\n",
    "\n",
    "confounds=pandas.read_csv(os.path.join(sesdir,\n",
    "                        'sub-MSC01_ses-func%02d_task-rest_bold_confounds.tsv'%ses),\n",
    "                         sep='\\t',na_values='n/a')\n",
    "confounds=confounds.replace(numpy.nan,0)\n",
    "# add temporal derivatives of motion estimates\n",
    "motionvars=['X', 'Y', 'Z', 'RotX', 'RotY', 'RotZ']\n",
    "for v in motionvars:\n",
    "    confounds['%s_deriv'%v]=0\n",
    "    confounds['%s_deriv'%v].iloc[1:]=confounds[v].iloc[1:].values - confounds[v].iloc[:-1].values\n",
    "\n",
    "print('confound variables:',confounds.columns)\n",
    "\n",
    "plt.imshow(sklearn.preprocessing.scale(confounds).T,aspect='auto',\n",
    "           cmap='viridis',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data across all surface vertices next a measure of motion (Framewise Displacement).  This shows that motion tends to induce fairly large global signal changes, which persist even after motion correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(sklearn.preprocessing.scale(bolddata_orig).T,aspect='auto',\n",
    "           cmap='gray',interpolation='nearest')\n",
    "plt.xlabel('timepoints')\n",
    "plt.ylabel('vertices')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "plt.plot(confounds.FramewiseDisplacement)\n",
    "plt.axis([0,bolddata_orig.shape[0],0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most controversial aspects of resting fMRI data is whether one should regress out the global signal (i.e. the average signal across the whole brain).  Our approach is generally to perform any analyses with and without global signal regression (GSR) and ensure that any findings of interest exist in both analyses.  Here let's perform the confound regression.  Note that it is generally important to perform all regression and filtering operations in a single step - otherwise one can end up re-introducing artifactual signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confounds with GSR\n",
    "confounds_to_include = ['CSF', 'WhiteMatter', 'GlobalSignal', 'stdDVARS',\n",
    "                        'FramewiseDisplacement', 'tCompCor00', 'tCompCor01',\n",
    "                        'tCompCor02', 'tCompCor03', 'tCompCor04', 'tCompCor05', \n",
    "                        'aCompCor00','aCompCor01', 'aCompCor02', 'aCompCor03', \n",
    "                        'aCompCor04', 'aCompCor05',\n",
    "                       'Cosine00', 'Cosine01', 'Cosine02', 'Cosine03', 'Cosine04', 'Cosine05',\n",
    "                       'Cosine06', 'Cosine07', 'Cosine08', 'Cosine09', 'Cosine10', 'Cosine11',\n",
    "                       'Cosine12', 'Cosine13', 'Cosine14', 'Cosine15', 'Cosine16', 'Cosine17',\n",
    "                       'Cosine18', 'Cosine19', 'Cosine20', 'Cosine21', 'Cosine22', 'Cosine23',\n",
    "                       'Cosine24', 'Cosine25', 'Cosine26', 'X', 'Y', 'Z', 'RotX', 'RotY', 'RotZ',\n",
    "                       'X_deriv', 'Y_deriv', 'Z_deriv', 'RotX_deriv', 'RotY_deriv',\n",
    "                       'RotZ_deriv']\n",
    "confounds_gsr=confounds[confounds_to_include]\n",
    "\n",
    "confounds_nogsr=confounds_gsr.copy()\n",
    "del confounds_nogsr['GlobalSignal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform confound regression with and without GSR\n",
    "\n",
    "lr=sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "lr.fit(confounds_gsr.values,bolddata_orig)\n",
    "bolddata_reg_gsr = bolddata_orig - lr.predict(confounds_gsr)\n",
    "\n",
    "lr.fit(confounds_nogsr.values,bolddata_orig)\n",
    "bolddata_reg_nogsr = bolddata_orig - lr.predict(confounds_nogsr)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the vertex-wise data for both of these, which shows that the data without GSR still seem to show some global fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.imshow(bolddata_reg_gsr.T-bolddata_reg_nogsr.T,\n",
    "           aspect='auto',cmap='gray',\n",
    "          interpolation='nearest')\n",
    "plt.xlabel('timepoints')\n",
    "plt.ylabel('vertices')\n",
    "plt.title('difference between GSR and noGSR')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting data from parcellation\n",
    "\n",
    "In general we don't work with the data from each voxel/vertex, because (as we will discuss in more detail later in the course) there seems to be clear low-dimensional structure in the data.  Instead, we usually average over a smaller number of *parcels* that we think are related to one another.  For the present analysis we will use a group parcellation generated by Glasser et al. from the Human Connectome Project data. Let's extract the signal for all of the parcels from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlasdir='../data/HCP-MMP1'\n",
    "atlas={'L':'lh.HCP-MMP1.fsaverage5.gii','R':'rh.HCP-MMP1.fsaverage5.gii'}\n",
    "atlasdata={}\n",
    "atlaslabels={}\n",
    "for a in atlas:\n",
    "   atlaslabeltable=nibabel.load(os.path.join(atlasdir,atlas[a])).labeltable.labels\n",
    "   atlaslabels[a]=[i.label for i in atlaslabeltable[1:]]\n",
    "   atlasdata[a]=nibabel.load(os.path.join(atlasdir,atlas[a])).darrays[0].data \n",
    "allatlaslabels=atlaslabels['L']+atlaslabels['R']\n",
    "allatlasdata=numpy.hstack((atlasdata['L'],atlasdata['R']+180))\n",
    "                          \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each hemisphere/view\n",
    "\n",
    "for i,hemi in enumerate(['left','right']):\n",
    "    for view in ['lateral','medial']:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        nilearn.plotting.plot_surf_stat_map(fsaverage['infl_%s'%hemi], \n",
    "                                    atlasdata[hemispheres[i]],\n",
    "                                    thresh=0.5,\n",
    "                                    hemi=hemi,\n",
    "                                   view=view,cmap='prism')\n",
    "        plt.title('hemi=%s,view=%s'%(hemi,view))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the mean timeseries from each of the parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roidata_gsr=numpy.zeros((ntp,361)) # 360 ROIs by 818 timepoints\n",
    "roidata_nogsr=numpy.zeros((ntp,361)) # 360 ROIs by 818 timepoints\n",
    "\n",
    "                        \n",
    "for region in range(361):\n",
    "    if region==0:\n",
    "        continue\n",
    "    regionverts=allatlasdata==region\n",
    "    for tp in range(ntp):\n",
    "      tmp=bolddata_reg_gsr[tp,:]\n",
    "      roidata_gsr[tp,region]=numpy.mean(tmp[regionverts])\n",
    "      tmp=bolddata_reg_nogsr[tp,:]\n",
    "      roidata_nogsr[tp,region]=numpy.mean(tmp[regionverts])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion scrubbing\n",
    "\n",
    "In general, the effects of motion will persist even after the use of confound regression. For this reason, many investigators will censor (or *scrub*) the data by removing timepoints that exceed some particular threshold for motion, along with a number of following timepoints (since the effects of motion often persist for several timepoints).  \n",
    "\n",
    "One common strategy is to identify timepoints that exceed a particular FD threshold (an aggressive choice advocated by Power and colleagues is FD>0.2; a less aggressive threshold commonly used is 0.5) along with some number of timepoints that follow those (with 10 being an aggressive number).  Note that scrubbing will interfere with analyses that require the full timeseries, but for simple correlations it doesn't cause any problems. Let's generate the temporal mask to identify and remove bad timeseries - we will use a less aggressive threshold but an aggressive window for removal of timepoints following motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_thresh=0.5\n",
    "tps_exceeding_fd_thresh=numpy.where(confounds.FramewiseDisplacement.values>fd_thresh)\n",
    "tswindow=10\n",
    "tsmask=numpy.ones(confounds.shape[0])\n",
    "for tp in tps_exceeding_fd_thresh[0]:\n",
    "    tsmask[(tp-1):(tp+tswindow)]=0\n",
    "\n",
    "print('%d good timepoints remaining after scrubbing (%d removed)'%(numpy.sum(tsmask),\n",
    "                                                                   ntp-numpy.sum(tsmask)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we censor the masked timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roidata_gsr_scrubbed=roidata_gsr[numpy.where(tsmask)[0],:]\n",
    "roidata_nogsr_scrubbed=roidata_nogsr[numpy.where(tsmask)[0],:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data from one parcel (in the left posterior cingulate cortex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcelnum=30\n",
    "print('using region %d: %s'%(parcelnum,atlaslabels['L'][parcelnum]))\n",
    "f,ax=plt.subplots(1,2,subplot_kw={\"projection\": \"3d\"},figsize=(12,6))\n",
    "ax[0].view_init(270,270)\n",
    "ntp_to_plot=100\n",
    "ax[0].plot(range(ntp_to_plot),roidata_gsr[:ntp_to_plot,parcelnum],zs=0)\n",
    "\n",
    "nilearn.plotting.plot_surf_roi(fsaverage['infl_left'], \n",
    "                                    (atlasdata['L']==parcelnum).astype('int'),thresh=0.5,\n",
    "                                    hemi='left',bg_map=fsaverage['sulc_left'],\n",
    "                                   view='medial',cmap='prism',axes=ax[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed correlation\n",
    "\n",
    "Let's compute the correlation between that particular parcel and all other parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standardize each parcel over time, so that we can compute correlation using standard linear regression\n",
    "roidata_gsr_scrubbed_z=sklearn.preprocessing.scale(roidata_gsr_scrubbed)\n",
    "lr=sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "lr.fit(roidata_gsr_scrubbed_z[:,parcelnum].reshape(-1,1),roidata_gsr_scrubbed_z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a new gifti image containing the correlation values, so we can visualize them\n",
    "\n",
    "# first we need to map the parcel data back into vertices\n",
    "corrdata_vertices=numpy.zeros(allatlasdata.shape)\n",
    "for i in list(numpy.unique(allatlasdata)):\n",
    "    corrdata_vertices[allatlasdata==i]=lr.coef_[int(i)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrimg={}\n",
    "hemis_text=['left','right']\n",
    "for i,h in enumerate(hemispheres):\n",
    "    corrimg[h]=GiftiImage(header=nibabel.load(fsaverage['infl_%s'%hemis_text[i]]).header)\n",
    "    corrimg[h].add_gifti_data_array(GiftiDataArray(corrdata_vertices[i*nverts:(i+1)*nverts]))\n",
    "\n",
    "f,ax=plt.subplots(2,2,subplot_kw={\"projection\": \"3d\"},figsize=(12,12))\n",
    "\n",
    "for i,h in enumerate(hemispheres):\n",
    "    for j,view in enumerate(['lateral','medial']):\n",
    "        nilearn.plotting.plot_surf_stat_map(fsaverage['infl_%s'%hemis_text[i]],\n",
    "                                            corrimg[h].darrays[0].data,\n",
    "                                           view=view,axes=ax[i,j],\n",
    "                                            colorbar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole-brain correlation\n",
    "\n",
    "Now let's compute the correlation between all parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscorr_gsr=numpy.corrcoef(roidata_gsr_scrubbed[:,1:].T) # drop the zero roi data\n",
    "tscorr_nogsr=numpy.corrcoef(roidata_nogsr_scrubbed[:,1:].T) # drop the zero roi data\n",
    "\n",
    "# we end up with a few NAN values because of an empty ROI, for now just zero them out\n",
    "tscorr_gsr[numpy.isnan(tscorr_gsr)]=0\n",
    "tscorr_nogsr[numpy.isnan(tscorr_nogsr)]=0\n",
    "\n",
    "plt.imshow(tscorr_gsr)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear from looking at the data that there is substantial structure in the correlations.  To see this more clearly, let's reorganize the data using the network assignments that were given to the regions on the basis of resting state correlation structurea by Yeo et al. (2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created previously using get_yeo_assignments.py\n",
    "labelfile=os.path.join(atlasdir,'MMP_yeo2011_networks.csv')\n",
    "labeldata=pandas.read_csv(labelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_corrs(corrmtx,labeldata,labels='YeoDesc7'):\n",
    "    \"\"\"\n",
    "    reorder correlation matrix according to network labels\n",
    "    \"\"\"\n",
    "    \n",
    "    idx=numpy.lexsort(([i for i in range(labeldata.shape[0])],labeldata[labels]))\n",
    "    tmp=corrmtx[:,idx]\n",
    "    return(tmp[idx,:],labeldata.iloc[idx,:])\n",
    "\n",
    "def plot_reordered_corrs(corrmtx,labeldata,labels='YeoDesc7',colorbar=True):\n",
    "    \"\"\"\n",
    "    plot correlation matrix after reordering\n",
    "    \"\"\"\n",
    "\n",
    "    corr_reord,labeldata_reord=reorder_corrs(corrmtx,labeldata)\n",
    "    plt.imshow(corr_reord)\n",
    "    # find breakpoints and plot lines\n",
    "    breaks=numpy.array([int(not i) for i in labeldata_reord[labels].values[:-1]==labeldata_reord[labels].values[1:]])\n",
    "    breaklocs=numpy.hstack((numpy.where(breaks)[0],numpy.array(corrmtx.shape[0]-1)))\n",
    "    for b in breaklocs:\n",
    "        plt.plot([0,corrmtx.shape[0]-1],[b,b],color='w',linewidth=0.5)\n",
    "        plt.plot([b,b],[0,corrmtx.shape[0]-1],color='w',linewidth=0.5)\n",
    "    # find label locations\n",
    "    # add a zero to help find label locations \n",
    "    breaklocs2=numpy.hstack(([0],breaklocs))\n",
    "    label_locs=numpy.mean(numpy.vstack((breaklocs,breaklocs2[:-1])),0)\n",
    "    networks=labeldata_reord[labels].values[breaklocs]\n",
    "    ax=plt.gca()\n",
    "    ax.set_yticks(label_locs)\n",
    "    ax.set_yticklabels(networks)\n",
    "    if colorbar:\n",
    "        plt.colorbar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_reordered_corrs(tscorr_gsr,labeldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of global signal regression\n",
    "\n",
    "There is often a substantial global component in resting state data, which in many cases is due to motion or breathing (e.g. [Power et al., 2017](https://www.ncbi.nlm.nih.gov/pubmed/27751941)). Let's plot the histogram of correlations to see how they are changed by GSR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_gsr=numpy.histogram(tscorr_gsr[numpy.triu_indices_from(tscorr_gsr,1)],numpy.arange(-1,1,0.01))\n",
    "h_nogsr=numpy.histogram(tscorr_nogsr[numpy.triu_indices_from(tscorr_gsr,1)],numpy.arange(-1,1,0.01))\n",
    "\n",
    "plt.plot(numpy.arange(-0.995,0.995,0.01),h_gsr[0])\n",
    "plt.plot(numpy.arange(-0.995,0.995,0.01),h_nogsr[0],color='red')\n",
    "plt.xlabel('correlation value')\n",
    "plt.ylabel('count')\n",
    "plt.legend(['GSR','no GSR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that GSR substantially shifts the distribution of correlations towards the left.  Now let's compare the correlation values with and without GSR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(tscorr_gsr[numpy.triu_indices_from(tscorr_gsr,1)],\n",
    "        tscorr_nogsr[numpy.triu_indices_from(tscorr_gsr,1)],)\n",
    "plt.plot([-1,1],[-1,1],'k-')\n",
    "plt.axis([-1,1,-1,1])\n",
    "plt.xlabel('r(GSR)')\n",
    "plt.ylabel('r(no GSR)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that GSR generally reduces correlation values across the entire distribution of correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network analysis\n",
    "\n",
    "Now let's generate a graph based on the functional connectivity data that we generated above.  We will start by thresholding the correlation matrix to generate an adjacency matrix, and then generating a graph from that. Thresholding is necessarily arbitrary; we will choose the top 5% of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjmtx(corrmtx,density,verbose=False):\n",
    "    assert density<=1\n",
    "    cutoff=scipy.stats.scoreatpercentile(corrmtx[numpy.triu_indices_from(corrmtx,1)],\n",
    "                                         100-(100*density))\n",
    "    if verbose:\n",
    "        print('cutoff:%0.3f'%cutoff)\n",
    "    adjmtx=(corrmtx>cutoff).astype('int')\n",
    "    adjmtx[numpy.diag_indices_from(adjmtx)]=0\n",
    "    return(adjmtx)\n",
    "\n",
    "adjmtx=get_adjmtx(tscorr_gsr,.025,verbose=True)\n",
    "plt.imshow(adjmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate a graph using NetworkX\n",
    "adjmtx=get_adjmtx(tscorr_gsr,.025,verbose=True)\n",
    "G=nx.from_numpy_array(adjmtx)\n",
    "\n",
    "# get giant component\n",
    "comps=[i for i in nx.connected_component_subgraphs(G)]\n",
    "Gc=comps[0]\n",
    "print('Giant component includes %d out of %d total nodes'%(len(Gc.nodes),len(G.nodes)))\n",
    "labeldata_Gc=labeldata.loc[list(Gc.nodes)]\n",
    "\n",
    "\n",
    "cl={0:'black',1:'red',2:'yellow',3:'green',4:'blue',5:'orange',6:'gray',7:'magenta'}\n",
    "colors=[cl[labeldata['Yeo7'].iloc[i]] for i in Gc.nodes]\n",
    "degrees=numpy.array([Gc.degree(i) for i in Gc.nodes])\n",
    "expansion=3\n",
    "layout=nx.spring_layout(Gc)\n",
    "nx.draw_networkx(Gc,pos=layout,with_labels=False,node_color=colors,\n",
    "              node_size=degrees*expansion)\n",
    "_=plt.axis('off')\n",
    "yeodict={0:'Undefined',1:'Visual',2:'Somatomotor',3:'DorsalAttention',\n",
    "         4:'VentralAttention',5:'Limbic',\n",
    "         6:'Frontoparietal',7:'Default'}\n",
    "\n",
    "for i in yeodict:\n",
    "    print(cl[i],':',yeodict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection\n",
    "\n",
    "Now let's run our own community detection analysis on the data, to see how the inferred communities relate to the established Yeo et al. networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density=0.05\n",
    "# get adj matrix for giant component\n",
    "Gc_nodelist=list(Gc.nodes)\n",
    "tmp=tscorr_gsr[Gc_nodelist,:]\n",
    "tscorr_gsr_Gc=tmp[:,Gc_nodelist]\n",
    "adjmtx=get_adjmtx(tscorr_gsr_Gc,density)\n",
    "\n",
    "mod_binary=bct.modularity_louvain_und(adjmtx)\n",
    "print('modularity:',mod_binary[1])\n",
    "\n",
    "print('Multilevel modularity optimization identifed %d communities'%len(numpy.unique(mod_binary[0])))\n",
    "ari=sklearn.metrics.adjusted_rand_score(mod_binary[0],\n",
    "                                        labeldata_Gc['Yeo7'])\n",
    "print('Adjusted Rand index compared to Yeo 7 networks: %0.3f'%ari)\n",
    "\n",
    "nx.draw_networkx(Gc,pos=layout,with_labels=False,\n",
    "            node_color=[mod_binary[0][i] for i in range(len(Gc.nodes))],\n",
    "            node_size=degrees*expansion,cmap='viridis')\n",
    "\n",
    "_=plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted/signed networks\n",
    "\n",
    "When we binarize a correlation to create an adjacency matrix, we are throwing away important information about negatively signed relationships, which we think are important in fMRI data.  There are generalizations of community detection methods that we can use for weighted and signed networks that can try to take advantage of this information.  Here we use a version of modularity optimization adapted from the [Brain Connectivity Toolbox](https://sites.google.com/site/bctnet/) (using the Python adaptation from https://github.com/aestrivex/bctpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_signed_weighted=bct.modularity_louvain_und_sign(tscorr_gsr_Gc)\n",
    "print('signed/weighted analysis found %d communities (Q=%0.3f)'%(len(numpy.unique(mod_signed_weighted[0])),\n",
    "                                                                 mod_signed_weighted[1]))\n",
    "ari_yeo_ms=sklearn.metrics.adjusted_rand_score(mod_signed_weighted[0],[labeldata['Yeo7'].iloc[i] for i in Gc.nodes])\n",
    "print('Adjusted Rand index compared to Yeo 7 networks: %0.3f'%ari_yeo_ms)\n",
    "\n",
    "print('')\n",
    "print('binary analysis found %d communities (Q=%0.3f) at density=%0.3f'%(len(numpy.unique(mod_binary[0])),\n",
    "                                                                 mod_binary[1],density))\n",
    "\n",
    "ari=sklearn.metrics.adjusted_rand_score(mod_signed_weighted[0],mod_binary[0])\n",
    "\n",
    "ari_yeo_bin=sklearn.metrics.adjusted_rand_score(mod_binary[0],[labeldata['Yeo7'].iloc[i] for i in Gc.nodes])\n",
    "print('Adjusted Rand index compared to Yeo 7 networks: %0.3f'%ari_yeo_bin)\n",
    "print('')\n",
    "print('Adjusted Rand index for weighted/signed vs binary: %0.3f'%ari)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial correlation\n",
    "\n",
    "Correlation is sensitive both to direct connections between regions as well as indirect connections.  Instead we might be interested in direct connections only, which can be estimated using partial correlation (also known as inverse covariance). Let's estimate that using the graphical lasso technique, which uses L1 regularization that imposes sparsity on the solution (i.e. many of the entries are zero).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset has one roi with no variance, remove it before the analysis\n",
    "roidata_gsr_scrubbed_nozero=roidata_gsr_scrubbed[:,numpy.arange(roidata_gsr_scrubbed.shape[1])!=120]\n",
    "\n",
    "estimator=nilearn.connectome.ConnectivityMeasure(kind='partial correlation')\n",
    "pcorr=estimator.fit_transform([roidata_gsr_scrubbed_nozero[:,1:]])   # drop the zero-ROI data\n",
    "pcorr=pcorr[0]\n",
    "pcorr[numpy.diag_indices_from(pcorr)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldata_cleaned=labeldata.copy()\n",
    "labeldata_cleaned=labeldata_cleaned.drop(119)\n",
    "plot_reordered_corrs(pcorr,labeldata_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing functional connectivity using matrix factorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "fica=FastICA(n_components=40)\n",
    "comps=fica.fit_transform(roidata_gsr_scrubbed.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_component(comps,compnum,\n",
    "                        surf_meshfile,surf_sulcfile,\n",
    "                        parcelmask,\n",
    "                        hemispheres=['L','R'],\n",
    "                       views=['lateral','medial'],\n",
    "                       thresh=95):\n",
    "    \n",
    "    comp=comps[:,compnum]\n",
    "    if thresh is not None:\n",
    "        cutoff=scipy.stats.scoreatpercentile(comp,thresh)\n",
    "    else:\n",
    "        cutoff=0\n",
    "    # first map from parcel space to vertex space\n",
    "    parcel_ids=numpy.unique(parcelmask) \n",
    "    nvertices=int(len(parcelmask)/2)\n",
    "    data=numpy.zeros(parcelmask.shape)\n",
    "    for i in list(parcel_ids):\n",
    "        data[parcelmask==i]=comp[int(i)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # display vertex space data\n",
    "    f,ax=plt.subplots(2,2,subplot_kw={\"projection\": \"3d\"},figsize=(8,8))\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "\n",
    "    for i,h in enumerate(hemispheres):\n",
    "        for j,view in enumerate(views):\n",
    "            nilearn.plotting.plot_surf_stat_map(fsaverage['infl_%s'%hemis_text[i]],\n",
    "                                                data[i*nvertices:(i+1)*nvertices],\n",
    "                                               view=view,axes=ax[i,j],\n",
    "                                               bg_map=surf_sulcfile[h],\n",
    "                                               threshold=cutoff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_meshfile={'L':fsaverage['infl_left'],'R':fsaverage['infl_right']}\n",
    "surf_sulcfile={'L':fsaverage['sulc_left'],'R':fsaverage['sulc_right']}\n",
    "\n",
    "visualize_component(comps,1,surf_meshfile,surf_sulcfile,allatlasdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
