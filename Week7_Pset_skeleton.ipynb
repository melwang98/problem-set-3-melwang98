{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set: Week 7 (Resting fMRI)\n",
    "In this problem set you will load the correlation data from one of the Midnight Scan Club subjects (which has already been extracted using the Glasser MMP parcellation) and perform several analyses to characterize the network.\n",
    "\n",
    "As before, skeletal code is provided - please fill in any areas where you see ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,pickle,sys\n",
    "import numpy,pandas\n",
    "import nilearn.datasets\n",
    "import nilearn.plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import networkx as nx\n",
    "import sklearn.metrics\n",
    "import bct\n",
    "from collections import Counter\n",
    "\n",
    "from brainnetworks.r2z import r_to_z,z_to_r\n",
    "%matplotlib inline\n",
    "\n",
    "datadir = nilearn.datasets.get_data_dirs()[0]\n",
    "if not os.path.exists(datadir):\n",
    "    os.mkdir(datadir)\n",
    "    \n",
    "atlasdir='/home/vagrant/brain-networks-course/data/HCP-MMP1'\n",
    "\n",
    "labelfile=os.path.join(atlasdir,'MMP_yeo2011_networks.csv')\n",
    "labeldata=pandas.read_csv(labelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load the data\n",
    "\n",
    "sub=1\n",
    "corrtype='gsr'  # use data with global signal regression\n",
    "scrubtype='full' # don't use scrubbing\n",
    "\n",
    "\n",
    "subdir=os.path.join(datadir,'MSC/ds000224/derivatives/fmriprep/sub-MSC%02d/'%sub)\n",
    "\n",
    "corrs=pickle.load(open(os.path.join(subdir,'sub-MSC%02d_task-rest_corrmtx.pkl'%sub),'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the mean correlation matrix across sesssions, using the r-to-z transform to first convert them to Z scores and then convert back to r values after averaging. The correlation data are stored in a dictionary, with the following key structure:\n",
    "\n",
    "> ```corrs[session num][corrtype:{'gsr','nogsr'}][scrubtype:{'scrubbed','full'}]```\n",
    "\n",
    "We will use corrtype and scrubtype as specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/anaconda3/envs/py3/lib/python3.6/site-packages/brainnetworks/r2z.py:9: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  z=0.5*numpy.log((1.0+r)/(1.0-r))\n"
     ]
    }
   ],
   "source": [
    "corrsum=numpy.zeros(corrs[1][corrtype][scrubtype].shape)\n",
    "for s in corrs:\n",
    "    sesscor=corrs[s][corrtype][scrubtype]\n",
    "    corrsum+=r_to_z(sesscor)\n",
    "    \n",
    "meancorr=z_to_r(corrsum/len(corrs))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**: Create a binary adjacency matrix with a density of 5%, and use this to create a NetworkX graph.  Be sure to do the following:\n",
    "\n",
    "- exclude the diagonal when computing the cutoff \n",
    "- zero out the diagonal before creating the graph\n",
    "- extract the giant component from the graph (calling the resulting variable ```Gc```)\n",
    "- print the number of nodes in the giant component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Giant component includes 356 out of 360 total nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/anaconda3/envs/py3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "# determine cutoff for 5% density of the correlation matrix\n",
    "# using just the upper triangle of the matrix\n",
    "thresh=95  # in percent\n",
    "cutoff=scipy.stats.scoreatpercentile(meancorr[numpy.triu_indices_from(meancorr,1)],\n",
    "                                         100-(100*0.05))\n",
    "\n",
    "#create symmetric binary adjacency matrix\n",
    "# be sure to convert to integer\n",
    "adjmtx=(meancorr>cutoff).astype('int')\n",
    "\n",
    "# zero out the diagonal in the adjmtx\n",
    "adjmtx[numpy.diag_indices_from(adjmtx)]=0\n",
    "\n",
    "# Create numpy graph\n",
    "G=nx.from_numpy_array(adjmtx)\n",
    "\n",
    "# create graph for giant component\n",
    "# first get all component subgraphs\n",
    "comps=[i for i in nx.connected_component_subgraphs(G)]\n",
    "# then take the largest\n",
    "Gc=comps[0]\n",
    "\n",
    "print('Giant component includes %d out of %d total nodes'%(len(Gc.nodes),len(G.nodes)))\n",
    "\n",
    "#Gc adjacency matrix\n",
    "adjmtxGc=nx.adjacency_matrix(Gc, nodelist=None, weight='weight')\n",
    "\n",
    "\n",
    "# grab the label data for only the nodes in the giant component\n",
    "labeldata_Gc=labeldata.loc[list(Gc.nodes)]\n",
    "# add degree values to labeldata frame\n",
    "labeldata_Gc['degree']=[Gc.degree[i] for i in labeldata_Gc.index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3**: Perform community detection on the graph, using the Louvain algorithm for undirected binary graphs as implemented in the bct python package, and compute their overlap with the Yeo 7 network parcellation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8e2c6d4e153f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute modularity using bct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmod_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodularity_louvain_und\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjmtxGc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'modularity:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Multilevel modularity optimization identifed %d communities'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/bctpy-0.5.0-py3.6.egg/bct/algorithms/modularity.py\u001b[0m in \u001b[0;36mmodularity_louvain_und\u001b[0;34m(W, gamma, hierarchy, seed)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# weight of edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# hierarchy index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# non-zeros is more important.  For now, raise an exception!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         raise TypeError(\"sparse matrix length is ambiguous; use getnnz()\"\n\u001b[0m\u001b[1;32m    297\u001b[0m                         \" or shape[0]\")\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "# compute modularity using bct\n",
    "mod_binary=bct.modularity_louvain_und(adjmtxGc)\n",
    "\n",
    "print('modularity:',mod_binary[1])\n",
    "print('Multilevel modularity optimization identifed %d communities'%len(numpy.unique(mod_binary[0])))\n",
    "\n",
    "# compute adjusted rand score using method from sklearn.metrics\n",
    "ari=sklearn.metrics.adjusted_rand_score(mod_binary[0],labeldata_Gc['Yeo7'])\n",
    "print('Adjusted Rand index compared to Yeo 7 networks: %0.3f'%ari)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**: Estimate the normalized rich club coefficient for this network and plot the coefficients across the range of degree values.  Find the smallest degree value  for which the rich club coefficient is greater than 2, which we will use to define the rich club nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree_cutoff: 35\n",
      "13 nodes in rich club\n"
     ]
    }
   ],
   "source": [
    "# embed computation of rcc within a try/catch since it fails\n",
    "# pretty regularly with a ZeroDivisionError\n",
    "good_rcc=False\n",
    "while not good_rcc:\n",
    "    try:\n",
    "        rcc = nx.rich_club_coefficient(Gc,normalized=True,Q=10)\n",
    "        good_rcc=True\n",
    "    except ZeroDivisionError:\n",
    "        print('error, retrying')\n",
    "        \n",
    "# put into a data frame\n",
    "rccdata=pandas.DataFrame([(i,rcc[i]) for i in rcc.keys()],\n",
    "                         columns=['degree','rcc'])\n",
    "\n",
    "# find the degree cutoff for rcc >= 2\n",
    "degree_cutoff=numpy.min(rccdata[rccdata['rcc'] >= 2]['degree'])\n",
    "print('degree_cutoff:',degree_cutoff) \n",
    "\n",
    "# compute the size of the rich club\n",
    "rc_size=numpy.sum(rccdata['degree'] >= degree_cutoff)\n",
    "print(rc_size,'nodes in rich club')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5:** For each of the Yeo7 networks, determine how many rich club members fall within that network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Visual': 21, 'VentralAttention': 13, 'DorsalAttention': 6, 'Default': 2, 'Somatomotor': 1})\n"
     ]
    }
   ],
   "source": [
    "# first create a data frame containing label data just for rcc members\n",
    "\n",
    "labeldata_rcc=labeldata_Gc[labeldata_Gc['degree'] >= degree_cutoff]\n",
    "\n",
    "# use collections.Counter to generate a list of the counts of members in each\n",
    "# Yeo7 network\n",
    "c=Counter(labeldata_rcc['YeoDesc7'])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6:** First, compute the node betweenness centrality and edge betweeness centrality for the giant component network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute edge betweenness centrality\n",
    "ebc=nx.edge_betweenness_centrality(Gc)\n",
    "\n",
    "# compute node betweenness centrality\n",
    "bc=nx.betweenness_centrality(Gc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, compute the mean betweenness centrality for edges separated by whether they include 0, 1, or 2 members of the rich club, and print out the mean values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no members mean: 0.001303639459318467\n",
      "one members mean: 0.0010032096606917421\n",
      "two members mean: 0.0004747857984713991\n"
     ]
    }
   ],
   "source": [
    "Gc_rcc=Gc.copy()\n",
    "deg = dict(Gc_rcc.degree()) \n",
    "to_remove = [n for n in deg if deg[n] < degree_cutoff]\n",
    "Gc_rcc.remove_nodes_from(to_remove)\n",
    "richclubnodes=list(Gc_rcc.nodes)\n",
    "\n",
    "edges=ebc.keys()\n",
    "include_no_members=[]\n",
    "include_one_member=[]\n",
    "include_two_members=[]\n",
    "for edge in edges: \n",
    "    if edge[0] not in richclubnodes and edge[1] not in richclubnodes:\n",
    "        include_no_members.append(ebc[edge])\n",
    "    if (edge[0] in richclubnodes and edge[1] not in richclubnodes) or (edge[1] in richclubnodes and edge[0] not in richclubnodes):\n",
    "        include_one_member.append(ebc[edge])\n",
    "    if edge[0] in richclubnodes and edge[1] in richclubnodes:\n",
    "        include_two_members.append(ebc[edge])\n",
    "\n",
    "no_member_mean=numpy.mean(include_no_members)\n",
    "one_member_mean=numpy.mean(include_one_member)\n",
    "two_member_mean=numpy.mean(include_two_members)\n",
    "print('no members mean:', no_member_mean)\n",
    "print('one members mean:', one_member_mean)\n",
    "print('two members mean:', two_member_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the mean betweenness centrality for nodes, separated by whether the nodes are members of the rich club or not, and print the values for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 0.012539901048146873\n",
      "not in: 0.006572179058063817\n"
     ]
    }
   ],
   "source": [
    "inrichclubnodes=[]\n",
    "notinrichclubnodes=[]\n",
    "for node in list(Gc.nodes):\n",
    "    if node in richclubnodes:\n",
    "        inrichclubnodes.append(bc[node])\n",
    "    if node not in richclubnodes:\n",
    "        notinrichclubnodes.append(bc[node])\n",
    "in_mean=numpy.mean(inrichclubnodes)\n",
    "notin_mean=numpy.mean(notinrichclubnodes)\n",
    "print('in:', in_mean)\n",
    "print('not in:', notin_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does centrality of both nodes and edges relate to rich club membership?  Please explain (insert your answer in the following cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge centrality distinguishes between nodes in the rich club membership that are connected to edges that are most commonly taken in shortest paths, thus placing the connected nodes in the rich club. Node centrality distinguishes between nodes that are most passed through in paths, thus the ones that are most passed through are in the rich club. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
